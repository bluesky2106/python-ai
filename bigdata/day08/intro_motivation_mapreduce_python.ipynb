{"nbformat":4,"nbformat_minor":2,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"name":"python3","display_name":"Python 3.9.5 64-bit"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"name":"intro_motivation_mapreduce_python.ipynb","provenance":[],"collapsed_sections":["VEaY6aMwtl4w","erfHheistl4x","5kZAFCPstl46","xQpZUXBYtl46"]},"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"cells":[{"cell_type":"markdown","source":["\n","### Overview\n","\n","1. Recap of functional programming in Python\n","2. Python's `map` and `reduce` functions\n","3. Writing parallel code using `map`\n","4. The Map-Reduce programming model\n","\n"],"metadata":{"id":"VUlkWZxttl4n"}},{"cell_type":"markdown","source":["## History\n","\n","- The Map-Reduce programming model was popularised by Google (Dean and Ghemawat 2008).\n","\n","- The first popular open-source implementation was Apache Hadoop, first released in 2011.\n"],"metadata":{"id":"UnEtaYMetl4o"}},{"cell_type":"markdown","source":["## Functional programming\n","\n","Consider the following code:"],"metadata":{"id":"Er-nqtrStl4o"}},{"cell_type":"code","execution_count":1,"source":["def double_everything_in(data):\n","    result = []\n","    for i in data:\n","        result.append(2 * i)\n","    return result\n","\n","def quadruple_everything_in(data):\n","    result = []\n","    for i in data:\n","        result.append(4 * i)\n","    return result"],"outputs":[],"metadata":{"id":"KItrfbbMtl4p","executionInfo":{"status":"ok","timestamp":1629803767520,"user_tz":-420,"elapsed":5,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}}}},{"cell_type":"code","execution_count":2,"source":["double_everything_in([1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4, 6, 8, 10]"]},"metadata":{},"execution_count":2}],"metadata":{"id":"wJxh_EdStl4p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629803784769,"user_tz":-420,"elapsed":273,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"33d0b4bf-398c-4e97-b92c-0d71761060e4"}},{"cell_type":"code","execution_count":3,"source":["quadruple_everything_in([1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 8, 12, 16, 20]"]},"metadata":{},"execution_count":3}],"metadata":{"id":"NSJ9EoXXtl4r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629803791958,"user_tz":-420,"elapsed":371,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"55c0b182-9a7e-4211-9f9e-a6b9e4037c30"}},{"cell_type":"markdown","source":["### DRY - Fundamental Programming Concept\n","\n","- The above code violates the [\"do not repeat yourself\"](https://en.wikipedia.org/wiki/Don't_repeat_yourself_) principle of good software engineering practice.\n","\n","- How can rewrite the code so that it avoids duplication?"],"metadata":{"id":"QWAlQJ_9tl4s"}},{"cell_type":"code","execution_count":4,"source":["def multiply_by_x_everything_in(x, data):\n","    result = []\n","    for i in data:\n","        result.append(x * i)\n","    return result"],"outputs":[],"metadata":{"id":"ISBWzZdxtl4t","executionInfo":{"status":"ok","timestamp":1629803829995,"user_tz":-420,"elapsed":270,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}}}},{"cell_type":"code","execution_count":5,"source":["multiply_by_x_everything_in(2, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4, 6, 8, 10]"]},"metadata":{},"execution_count":5}],"metadata":{"id":"ktIZ1OZmtl4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629803833974,"user_tz":-420,"elapsed":429,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"c0b1f489-279d-4d39-a1db-5653d2011014"}},{"cell_type":"code","execution_count":6,"source":["multiply_by_x_everything_in(4, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 8, 12, 16, 20]"]},"metadata":{},"execution_count":6}],"metadata":{"id":"BUb8a_Uatl4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629803840931,"user_tz":-420,"elapsed":261,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"5f03b769-19ec-4f82-c7b2-3488bf821d86"}},{"cell_type":"markdown","source":["- Now consider the following code:"],"metadata":{"id":"CPY7p5Ebtl4u"}},{"cell_type":"code","execution_count":7,"source":["def squared(x):\n","    return x*x\n","\n","def double(x):\n","    return x*2\n","\n","def square_everything_in(data):\n","    result = []\n","    for i in data:\n","        result.append(squared(i))\n","    return result\n","\n","def double_everything_in(data):\n","    result = []\n","    for i in data:\n","        result.append(double(i))\n","    return result"],"outputs":[],"metadata":{"id":"DXoqZKbLtl4u","executionInfo":{"status":"ok","timestamp":1629803862182,"user_tz":-420,"elapsed":277,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}}}},{"cell_type":"code","execution_count":8,"source":["square_everything_in([1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25]"]},"metadata":{},"execution_count":8}],"metadata":{"id":"V8or_BFdtl4v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629803864537,"user_tz":-420,"elapsed":8,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"2e1b48a7-8183-45c3-f839-e1329dc6caf0"}},{"cell_type":"code","execution_count":9,"source":["double_everything_in([1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4, 6, 8, 10]"]},"metadata":{},"execution_count":9}],"metadata":{"id":"mpCMj4vPtl4v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629803870431,"user_tz":-420,"elapsed":373,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"7ec6014a-d8ce-4468-924f-fe1e039e2333"}},{"cell_type":"markdown","source":["### DRY - Fundamental Programming Concept\n","- The above code violates the [\"do not repeat yourself\"](https://en.wikipedia.org/wiki/Don't_repeat_yourself_) principle of good software engineering practice.\n","\n","- How can rewrite the code so that it avoids duplication?"],"metadata":{"id":"lojs3rlCtl4v"}},{"cell_type":"markdown","source":["### Passing Functions as Values\n","- Functions can be passed to other functions as values.\n","-\n"],"metadata":{"id":"VEaY6aMwtl4w"}},{"cell_type":"code","execution_count":10,"source":["def apply_f_to_everything_in(f, data):\n","    result = []\n","    for x in data:\n","        result.append(f(x))\n","    return result"],"outputs":[],"metadata":{"id":"607XT-K4tl4w","executionInfo":{"status":"ok","timestamp":1629803940259,"user_tz":-420,"elapsed":272,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}}}},{"cell_type":"code","execution_count":11,"source":["apply_f_to_everything_in(squared, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25]"]},"metadata":{},"execution_count":11}],"metadata":{"id":"SyaNOZyBtl4w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627953513408,"user_tz":-420,"elapsed":61,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"3a940165-fb84-46b4-a6bc-417d3f9dd6b8"}},{"cell_type":"code","execution_count":12,"source":["apply_f_to_everything_in(double, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4, 6, 8, 10]"]},"metadata":{},"execution_count":12}],"metadata":{"id":"IxvB3bnttl4w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629803982984,"user_tz":-420,"elapsed":330,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"c4bf60f2-9a77-42c2-ecf1-56bdafb9193c"}},{"cell_type":"markdown","source":["### Lambda expressions\n","\n","- We can use anonymous functions to save having to define a function each time we want to use map."],"metadata":{"id":"erfHheistl4x"}},{"cell_type":"code","execution_count":13,"source":["apply_f_to_everything_in(lambda x: x*x, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25]"]},"metadata":{},"execution_count":13}],"metadata":{"id":"1e8mkM2Ctl4x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804000830,"user_tz":-420,"elapsed":279,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"ac8a740c-9b26-45ec-b724-9f523168592f"}},{"cell_type":"markdown","source":["# Python's `map` function\n","\n","- Python has a built-in function `map` which is much faster than our version.\n","\n"],"metadata":{"id":"Ov5HqJHetl4x"}},{"cell_type":"code","execution_count":14,"source":["map(lambda x: x*x, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<map at 0x111afaa90>"]},"metadata":{},"execution_count":14}],"metadata":{"id":"Xthp90Nvtl4x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627953513410,"user_tz":-420,"elapsed":54,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"c2173e89-9bb4-4cb9-b611-09739107eede"}},{"cell_type":"markdown","source":["## Implementing reduce\n","\n","- The `reduce` function is an example of a [fold](https://en.wikipedia.org/wiki/Fold_%28higher-order_function%29).\n","\n","- There are different ways we can fold data.\n","\n","- The following implements a *left* fold.\n"],"metadata":{"id":"A5FhLeVWtl4y"}},{"cell_type":"code","execution_count":15,"source":["def foldl(f, data, z):\n","    if (len(data) == 0):\n","        print (z)\n","        return z\n","    else:\n","        head = data[0]\n","        tail = data[1:]\n","        print (\"Folding\", head, \"with\", tail, \"using\", z)\n","        partial_result = f(z, data[0])\n","        print (\"Partial result is\", partial_result)\n","        return foldl(f, tail, partial_result)  "],"outputs":[],"metadata":{"id":"2Rr0gPj_tl4y","executionInfo":{"status":"ok","timestamp":1629804153306,"user_tz":-420,"elapsed":307,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}}}},{"cell_type":"code","execution_count":16,"source":["def add(x, y):\n","    return x + y\n","\n","foldl(add, [3, 3, 3, 3, 3], 0)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Folding 3 with [3, 3, 3, 3] using 0\n","Partial result is 3\n","Folding 3 with [3, 3, 3] using 3\n","Partial result is 6\n","Folding 3 with [3, 3] using 6\n","Partial result is 9\n","Folding 3 with [3] using 9\n","Partial result is 12\n","Folding 3 with [] using 12\n","Partial result is 15\n","15\n"]},{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":16}],"metadata":{"id":"BffOLSjctl4y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804168978,"user_tz":-420,"elapsed":348,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"20d3eb8a-dd69-4ebf-c190-55c0f328d682"}},{"cell_type":"code","execution_count":17,"source":["foldl(lambda x, y: x + y, [1, 2, 3, 4, 5], 0)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Folding 1 with [2, 3, 4, 5] using 0\n","Partial result is 1\n","Folding 2 with [3, 4, 5] using 1\n","Partial result is 3\n","Folding 3 with [4, 5] using 3\n","Partial result is 6\n","Folding 4 with [5] using 6\n","Partial result is 10\n","Folding 5 with [] using 10\n","Partial result is 15\n","15\n"]},{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":17}],"metadata":{"id":"X2c8pZeWtl4y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804261674,"user_tz":-420,"elapsed":246,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"03ce929d-ab6e-48d2-a33e-6be6623afc22"}},{"cell_type":"code","execution_count":18,"source":["foldl(lambda x, y: x - y, [1, 2, 3, 4, 5], 0)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Folding 1 with [2, 3, 4, 5] using 0\n","Partial result is -1\n","Folding 2 with [3, 4, 5] using -1\n","Partial result is -3\n","Folding 3 with [4, 5] using -3\n","Partial result is -6\n","Folding 4 with [5] using -6\n","Partial result is -10\n","Folding 5 with [] using -10\n","Partial result is -15\n","-15\n"]},{"output_type":"execute_result","data":{"text/plain":["-15"]},"metadata":{},"execution_count":18}],"metadata":{"id":"wdMy-s1gtl4z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804269552,"user_tz":-420,"elapsed":247,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"275b426c-8175-4564-9d64-a70405b66eef"}},{"cell_type":"code","execution_count":19,"source":["(((((0 - 1) - 2) - 3) - 4) - 5)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["-15"]},"metadata":{},"execution_count":19}],"metadata":{"id":"9pEE26o3tl4z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804295859,"user_tz":-420,"elapsed":245,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"74527ffa-44d2-4965-c954-a09ffa28b13f"}},{"cell_type":"markdown","source":["- Subtraction is neither [commutative](https://en.wikipedia.org/wiki/Commutative_property) nor [associative](https://en.wikipedia.org/wiki/Associative_property), so the order in which apply the fold matters:"],"metadata":{"id":"P9RGnLIctl4z"}},{"cell_type":"code","execution_count":20,"source":["(1 - (2 - (3 - (4 - (5 - 0)))))"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":20}],"metadata":{"id":"oIfG7Zxntl4z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804313300,"user_tz":-420,"elapsed":277,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"79c7dca5-ac46-4d77-ef07-edd505ad7ddf"}},{"cell_type":"code","execution_count":21,"source":["def foldr(f, data, z):\n","    if (len(data) == 0):\n","        return z\n","    else:\n","        return f(data[0], foldr(f, data[1:], z))                "],"outputs":[],"metadata":{"id":"aMtG-dSZtl4z","executionInfo":{"status":"ok","timestamp":1629804327964,"user_tz":-420,"elapsed":278,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}}}},{"cell_type":"code","execution_count":22,"source":["foldl(lambda x, y: x - y,  [1, 2, 3, 4, 5], 0)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Folding 1 with [2, 3, 4, 5] using 0\n","Partial result is -1\n","Folding 2 with [3, 4, 5] using -1\n","Partial result is -3\n","Folding 3 with [4, 5] using -3\n","Partial result is -6\n","Folding 4 with [5] using -6\n","Partial result is -10\n","Folding 5 with [] using -10\n","Partial result is -15\n","-15\n"]},{"output_type":"execute_result","data":{"text/plain":["-15"]},"metadata":{},"execution_count":22}],"metadata":{"id":"-_XOcMintl40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804331600,"user_tz":-420,"elapsed":302,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"ee3b97c2-12ef-4f64-e7c7-72399a295af4"}},{"cell_type":"code","execution_count":23,"source":["foldr(lambda x, y: x - y, [1, 2, 3, 4, 5], 0)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":23}],"metadata":{"id":"Udv4ZFmXtl40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804349227,"user_tz":-420,"elapsed":282,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"68f44102-7c24-480a-b8e7-06e454ec7f0a"}},{"cell_type":"markdown","source":["## Python's `reduce` function.\n","\n","- Python's built-in `reduce` function is a *left* fold."],"metadata":{"id":"6qkagLEatl40"}},{"cell_type":"code","execution_count":22,"source":["from functools import reduce\n","reduce(lambda x, y: x + y, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":22}],"metadata":{"id":"z2wqZLH8tl40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804401838,"user_tz":-420,"elapsed":292,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"16b74e44-db6a-437f-e9d4-709a9e79880c"}},{"cell_type":"code","execution_count":23,"source":["reduce(lambda x, y: x - y, [1, 2, 3, 4, 5], 0)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["-15"]},"metadata":{},"execution_count":23}],"metadata":{"id":"Ydxb9yQYtl41","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804403655,"user_tz":-420,"elapsed":245,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"8d29b717-d294-41ae-ffbf-72e8d0982342"}},{"cell_type":"code","execution_count":24,"source":["foldl(lambda x, y: x - y, [1, 2, 3, 4, 5], 0)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Folding 1 with [2, 3, 4, 5] using 0\n","Partial result is -1\n","Folding 2 with [3, 4, 5] using -1\n","Partial result is -3\n","Folding 3 with [4, 5] using -3\n","Partial result is -6\n","Folding 4 with [5] using -6\n","Partial result is -10\n","Folding 5 with [] using -10\n","Partial result is -15\n","-15\n"]},{"output_type":"execute_result","data":{"text/plain":["-15"]},"metadata":{},"execution_count":24}],"metadata":{"id":"EChnfJgQtl41","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804408360,"user_tz":-420,"elapsed":347,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"5adcdfd2-6966-4524-daf9-be00d3e316d5"}},{"cell_type":"markdown","source":["# Functional programming and parallelism\n","\n","- Functional programming lends itself to [parallel programming](https://computing.llnl.gov/tutorials/parallel_comp/#Models).\n","\n","- The `map` function can easily be parallelised through [data-level parallelism](https://en.wikipedia.org/wiki/Data_parallelism),\n","    - provided that the function we supply as an argument is *free from* [side-effects](https://en.wikipedia.org/wiki/Side_effect_%28computer_science%29)\n","        - (which is why we avoid working with mutable data).\n","\n","- We can see this by rewriting it so:\n"],"metadata":{"id":"HfxFaiQgtl41"}},{"cell_type":"code","execution_count":25,"source":["def perform_computation(f, result, data, i):\n","    print (\"Computing the \", i, \"th result...\")\n","    # This could be scheduled on a different CPU\n","    result[i] = f(data[i])\n","\n","def my_map(f, data):\n","    result = [None] * len(data)\n","    for i in range(len(data)):\n","        perform_computation(f, result, data, i)\n","    # Wait for other CPUs to finish, and then..\n","    return result"],"outputs":[],"metadata":{"id":"0fmyo_mDtl41","executionInfo":{"status":"ok","timestamp":1629804448962,"user_tz":-420,"elapsed":252,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}}}},{"cell_type":"code","execution_count":26,"source":["my_map(lambda x: x * x, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"stream","name":"stdout","text":["Computing the  0 th result...\n","Computing the  1 th result...\n","Computing the  2 th result...\n","Computing the  3 th result...\n","Computing the  4 th result...\n"]},{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25]"]},"metadata":{},"execution_count":26}],"metadata":{"id":"ONvlsTx7tl41","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804493528,"user_tz":-420,"elapsed":271,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"eb912aa7-ae05-4381-f559-d4cdffd31406"}},{"cell_type":"markdown","source":["## A multi-threaded `map` function"],"metadata":{"id":"jit3MJu2tl42"}},{"cell_type":"code","execution_count":27,"source":["from threading import Thread\n","\n","def schedule_computation_threaded(f, result, data, threads, i):    \n","    # Each function evaluation is scheduled on a different core.\n","    def my_job(): \n","        print (\"Processing data:\", data[i], \"... \")\n","        result[i] = f(data[i])\n","        print (\"Finished job #\", i)    \n","        print (\"Result was\", result[i])       \n","    threads[i] = Thread(target=my_job)\n","    \n","def my_map_multithreaded(f, data):\n","    n = len(data)\n","    result = [None] * n\n","    threads = [None] * n\n","    print (\"Scheduling jobs.. \")\n","    for i in range(n):\n","        schedule_computation_threaded(f, result, data, threads, i)\n","    print (\"Starting jobs.. \")\n","    for i in range(n):\n","        threads[i].start()\n","    print (\"Waiting for jobs to finish.. \")\n","    for i in range(n):\n","        threads[i].join()\n","    print (\"All done.\")\n","    return result"],"outputs":[],"metadata":{"id":"FsT8BJrrtl42","executionInfo":{"status":"ok","timestamp":1629804622480,"user_tz":-420,"elapsed":297,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}}}},{"cell_type":"code","execution_count":28,"source":["my_map_multithreaded(lambda x: x*x, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"stream","name":"stdout","text":["Scheduling jobs.. \n","Starting jobs.. \n","Processing data: 1 ... \n","Finished job # 0\n","Result was Processing data: 1\n","2 ... \n","Finished job # 1\n","Result was 4\n","Processing data: 3 ... \n","Finished job # 2\n","Result was 9\n","Processing data: 4 ... \n","Finished job # 3\n","Result was 16\n","Processing data:Waiting for jobs to finish.. \n"," 5 ... \n","Finished job # 4\n","Result was 25\n","All done.\n"]},{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25]"]},"metadata":{},"execution_count":28}],"metadata":{"id":"gjcWZUAftl42","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804632623,"user_tz":-420,"elapsed":297,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"1fe8d3f3-9f2d-42e4-cd4e-d5a312cf059a"}},{"cell_type":"code","execution_count":29,"source":["from numpy.random import uniform\n","from time import sleep\n","\n","def a_function_which_takes_a_long_time(x):\n","    sleep(uniform(2, 10))  # Simulate some long computation\n","    return x*x\n","\n","my_map_multithreaded(a_function_which_takes_a_long_time, [1, 2, 3, 4, 5])"],"outputs":[{"output_type":"stream","name":"stdout","text":["Scheduling jobs.. \n","Starting jobs.. \n","Processing data: Processing data: 1 ... \n","2Processing data: ...  3 ... \n","\n","Processing data: 4 ... \n","Processing data: 5 ... \n","Waiting for jobs to finish.. \n","Finished job # 2\n","Result was 9\n","Finished job # 4\n","Result was 25\n","Finished job # 0\n","Result was 1\n","Finished job # 3\n","Result was 16\n","Finished job # 1\n","Result was 4\n","All done.\n"]},{"output_type":"execute_result","data":{"text/plain":["[1, 4, 9, 16, 25]"]},"metadata":{},"execution_count":29}],"metadata":{"id":"yEV4ea1Etl42","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629804696701,"user_tz":-420,"elapsed":10106,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"5827fd16-77f4-450a-b91b-752d4795f1f4"}},{"cell_type":"markdown","source":["## Map Reduce\n","\n","- Map Reduce is a _programming model_ for scalable parallel processing.\n","- Scalable here means that it can work on big data with very large compute clusters.\n","- There are many implementations: e.g. Apache Hadoop and Apache Spark.\n","- We can use Map-Reduce with any programming language:\n","    - Hadoop is written in Java\n","    - Spark is written in Scala, but has a Python interface.\n","- *Functional programming* languages such as Python or Scala fit very well with the Map Reduce model:\n","    - However, we don't *have* to use functional programming."],"metadata":{"id":"gx2Itg6Ptl42"}},{"cell_type":"markdown","source":["![MapReduce](https://github.com/pnavaro/big-data/blob/master/notebooks/images/mapreduce.jpg?raw=1)"],"metadata":{"id":"mFvetFDVD6e6"}},{"cell_type":"markdown","source":["- A MapReduce implementation will take care of the low-level functionality so that you don't have to worry about:\n","    - load balancing\n","    - network I/O\n","    - network and disk transfer optimisation\n","    - handling of machine failures\n","    - serialization of data\n","    - etc..\n","- The model is designed to move the processing to where the data resides."],"metadata":{"id":"WS28lTmatl42"}},{"cell_type":"markdown","source":["## Typical steps in a Map Reduce Computation\n","\n","1. ETL a big data set.\n","2. _Map_ operation: extract something you care about from each row\n","3. \"Shuffle and Sort\": task/node allocation\n","4. _Reduce_ operation: aggregate, summarise, filter or transform\n","5. Write the results."],"metadata":{"id":"UIrEYEnVtl43"}},{"cell_type":"markdown","source":["## Callbacks for Map Reduce\n","\n","- The data set, and the state of each stage of the computation, is represented as a set of key-value pairs.\n","\n","- The programmer provides a map function:\n","\n","$\\operatorname{map}(k, v) \\rightarrow \\; \\left< k', v' \\right>*$  \n","\n","- and a reduce function:\n","\n","$\\operatorname{reduce}(k', \\left< k', v'\\right> *) \\rightarrow \\; \\left< k', v''\n","\\right> *$\n","\n","- The $*$ refers to a *collection* of values.\n","\n","- These collections are *not* ordered."],"metadata":{"id":"etM5ZAYYtl43"}},{"cell_type":"markdown","source":["This part is to code in Python language a wordcount application using map-reduce process. A java version is well explained on [this page](https://www.dezyre.com/hadoop-tutorial/hadoop-mapreduce-wordcount-tutorial)\n","\n","![domain decomposition](https://github.com/pnavaro/big-data/blob/master/notebooks/images/domain_decomp.png?raw=1)\n","\n","credits: https://computing.llnl.gov/tutorials/parallel_comp"],"metadata":{"id":"ec-rrLrHDe2l"}},{"cell_type":"markdown","source":["## Word Count Example\n","\n","- In this simple example, the input is a set of URLs, each record is a document.\n","\n","- Problem: compute how many times each word has occurred across data set."],"metadata":{"id":"0FAXBh-Wtl43"}},{"cell_type":"markdown","source":["## Word Count: Map \n","\n","\n","- The input to $\\operatorname{map}$ is a mapping:\n","\n","- Key: URL\n","- Value: Contents of document"],"metadata":{"id":"PRL0h0Lhtl43"}},{"cell_type":"markdown","source":["$\\left< document1, to \\; be \\; or \\; not \\; to \\; be \\right>$  \n","    \n","\n","- In this example, our $\\operatorname{map}$ function will process a given URL, and produces a mapping:"],"metadata":{"id":"tbJefeRntl43"}},{"cell_type":"markdown","source":["- Key: word\n","- Value: 1\n","\n","- So our original data-set will be transformed to:\n","  \n","  $\\left< to, 1 \\right>$\n","  $\\left< be, 1 \\right>$\n","  $\\left< or, 1 \\right>$\n","  $\\left< not, 1 \\right>$\n","  $\\left< to, 1 \\right>$\n","  $\\left< be, 1 \\right>$"],"metadata":{"id":"IMtA3pz8tl43"}},{"cell_type":"markdown","source":["## Word Count: Reduce\n","\n","\n","- The reduce operation groups values according to their key, and then performs areduce on each key.\n","\n","- The collections are partitioned across different storage units, therefore.\n","\n","- Map-Reduce will fold the data in such a way that it minimises data-copying across the cluster.\n","\n","- Data in different partitions are reduced separately in parallel.\n","\n","- The final result is a reduce of the reduced data in each partition.\n","\n","- Therefore it is very important that our operator *is both commutative and associative*.\n","\n","- In our case the function is the `+` operator\n","\n","  $\\left< be, 2 \\right>$  \n","  $\\left< not, 1 \\right>$  \n","  $\\left< or, 1 \\right>$  \n","  $\\left< to, 2 \\right>$  \n","  "],"metadata":{"id":"O2qbbsrGtl43"}},{"cell_type":"markdown","source":["## Map and Reduce compared with Python\n","\n","- Notice that these functions are formulated differently from the standard Python functions of the same name.\n","\n","- The `reduce` function works with key-value *pairs*.\n","\n","- It would be more apt to call it something like `reduceByKey`.\n"],"metadata":{"id":"DWHFF8sEtl44"}},{"cell_type":"markdown","source":["## MiniMapReduce\n","\n","- To illustrate how the Map-Reduce programming model works, we can implement our own Map-Reduce framework in Python.\n","\n","- This *illustrates* how a problem can be written in terms of `map` and `reduce` operations.\n","\n","- Note that these are illustrative functions; this is *not* how Hadoop or Apache Spark actually implement them."],"metadata":{"id":"fkZt8lI4tl44"}},{"cell_type":"code","execution_count":null,"source":["##########################################################\n","#\n","#   MiniMapReduce\n","#\n","# A non-parallel, non-scalable Map-Reduce implementation\n","##########################################################\n","\n","def groupByKey(data):\n","    result = dict()\n","    for key, value in data:\n","        if key in result:\n","            result[key].append(value)\n","        else:\n","            result[key] = [value]\n","    return result\n","        \n","def reduceByKey(f, data):\n","    key_values = groupByKey(data)\n","    return map(lambda key: \n","                   (key, reduce(f, key_values[key])), \n","                       key_values)"],"outputs":[],"metadata":{"id":"pCXHugVwtl44"}},{"cell_type":"markdown","source":["## Word-count using MiniMapReduce\n"],"metadata":{"id":"EC0QrL7dtl44"}},{"cell_type":"code","execution_count":null,"source":["data = map(lambda x: (x, 1), \"to be or not to be\".split())\n","data"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<map at 0x7f21b81a6990>"]},"metadata":{"tags":[]},"execution_count":33}],"metadata":{"id":"z3LFtr7etl44","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627953520265,"user_tz":-420,"elapsed":60,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"228e6628-2dfc-425b-a42f-6598cee1627d"}},{"cell_type":"code","execution_count":null,"source":["groupByKey(data)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'be': [1, 1], 'not': [1], 'or': [1], 'to': [1, 1]}"]},"metadata":{"tags":[]},"execution_count":34}],"metadata":{"id":"KPGfpO66tl45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627953520267,"user_tz":-420,"elapsed":55,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"47de4751-ae6a-46e4-edec-0f349e0dfa2c"}},{"cell_type":"code","execution_count":null,"source":["reduceByKey(lambda x, y: x + y, data)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<map at 0x7f21b5f65f10>"]},"metadata":{"tags":[]},"execution_count":35}],"metadata":{"id":"Z2w3iWuUtl45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627953520268,"user_tz":-420,"elapsed":52,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"0750f74f-d1c4-46a4-f6e7-949109c40b7b"}},{"cell_type":"markdown","source":["## Parallelising MiniMapReduce\n","\n","- We can easily turn our Map-Reduce implementation into a parallel, multi-threaded framework\n","by using the `my_map_multithreaded` function we defined earlier.\n","\n","- This will allow us to perform map-reduce computations that exploit parallel processing using *multiple* cores on a *single* computer."],"metadata":{"id":"yPginmvJtl45"}},{"cell_type":"code","execution_count":null,"source":["def reduceByKey_multithreaded(f, data):\n","    key_values = groupByKey(data)\n","    return my_map_multithreaded(\n","        lambda key: (key, reduce(f, key_values[key])), key_values.keys())"],"outputs":[],"metadata":{"id":"iBbyFVlOtl45"}},{"cell_type":"code","execution_count":null,"source":["reduceByKey_multithreaded(lambda x, y: x + y, data)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Scheduling jobs.. \n","Starting jobs.. \n","Waiting for jobs to finish.. \n","All done.\n"]},{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{"tags":[]},"execution_count":37}],"metadata":{"id":"Ia87FJXAtl45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627953520271,"user_tz":-420,"elapsed":45,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"df97e4c2-2ed5-4143-ef3d-f1c47a0ad6ef"}},{"cell_type":"markdown","source":["## Parallelising the reduce step\n","\n","- Provided that our operator is both associative and commutative we can\n","also parallelise the reduce operation.\n","\n","- We partition the data into approximately equal subsets.\n","\n","- We then reduce each subset independently on a separate core.\n","\n","- The results can be combined in a final reduce step."],"metadata":{"id":"bIKtbfKvtl46"}},{"cell_type":"markdown","source":["### Partitioning the data"],"metadata":{"id":"5kZAFCPstl46"}},{"cell_type":"code","execution_count":null,"source":["def split_data(data, split_points):\n","    partitions = []\n","    n = 0\n","    for i in split_points:\n","        partitions.append(data[n:i])\n","        n = i\n","    partitions.append(data[n:])\n","    return partitions\n","\n","data = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\n","partitioned_data = split_data(data, [3])\n","partitioned_data"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['a', 'b', 'c'], ['d', 'e', 'f', 'g']]"]},"metadata":{"tags":[]},"execution_count":38}],"metadata":{"id":"M9bWNqmxtl46","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627953520272,"user_tz":-420,"elapsed":43,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"1b73abd1-5f1d-44a8-ecdd-6af01fd8021b"}},{"cell_type":"markdown","source":["### Reducing across partitions in parallel"],"metadata":{"id":"xQpZUXBYtl46"}},{"cell_type":"code","execution_count":null,"source":["from threading import Thread\n","\n","def parallel_reduce(f, partitions):\n","\n","    n = len(partitions)\n","    results = [None] * n\n","    threads = [None] * n\n","    \n","    def job(i):\n","        results[i] = reduce(f, partitions[i])\n","\n","    for i in range(n):\n","        threads[i] = Thread(target = lambda: job(i))\n","        threads[i].start()\n","    \n","    for i in range(n):\n","        threads[i].join()\n","    \n","    return reduce(f, results)\n","\n","parallel_reduce(lambda x, y: x + y, partitioned_data)"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["'abcdefg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{"tags":[]},"execution_count":39}],"metadata":{"id":"IU5D3SC_tl46","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1627953520273,"user_tz":-420,"elapsed":40,"user":{"displayName":"David Mai","photoUrl":"","userId":"08130445990978763482"}},"outputId":"b71af431-9650-4405-cad4-8422b7dfcdb3"}},{"cell_type":"markdown","source":["## Map-Reduce on a cluster of computers\n","\n","- The code we have written so far will *not* allow us to exploit parallelism from multiple computers in a [cluster](https://en.wikipedia.org/wiki/Computer_cluster).\n","\n","- Developing such a framework would be a very large software engineering project.\n","\n","- There are existing frameworks we can use:\n","    - [Apache Hadoop](https://hadoop.apache.org/)\n","    - [Apache Spark](https://spark.apache.org/)\n","    \n","- In our program, we will mostly focus on Apache Spark."],"metadata":{"id":"w2wM_1F2tl46"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"UF-FeXiiC_eM"}}]}