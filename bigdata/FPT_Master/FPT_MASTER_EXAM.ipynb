{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FPT_MASTER_EXAM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ITP7tiZkpYrT"},"source":["# This exam require you to perform simple exploration steps and simple machine learning pipeline on BitCoin Data. Please perform to your best to complete all the tasks given."]},{"cell_type":"markdown","metadata":{"id":"787iGYr8qFuc"},"source":["## 1) Setting up Pyspark environent on Google Colab\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2HF0NQ_lqQL0"},"source":["### 1. Installing dependencies"]},{"cell_type":"markdown","metadata":{"id":"K4TphPGNrrAs"},"source":["### 2. Assigning Environment Variables"]},{"cell_type":"markdown","metadata":{"id":"9dXdROw2sAte"},"source":["### 3. Launch a Spark Session and Spark Context with PySpark Library "]},{"cell_type":"markdown","metadata":{"id":"gQRastg4sMzr"},"source":["## 2) Accessing Data"]},{"cell_type":"markdown","metadata":{"id":"xmtbV1O-thtF"},"source":["### The problem is about Multivariate Timeseries Forecasting"]},{"cell_type":"markdown","metadata":{"id":"AS3nwIf5toWE"},"source":["### 1. Import Utilities/Data Science Libraries"]},{"cell_type":"markdown","metadata":{"id":"utfnLchwtvnd"},"source":["### 2. Importing data files. There are multiple csv files, read all and combine them into 1 spark dataframe. (Hint: you can use directly from pyspark read csv and merge them or create pandas df then create spark df from pandas)"]},{"cell_type":"markdown","metadata":{"id":"hQnDGxoAwNtO"},"source":["### 3. Exploring data structure"]},{"cell_type":"markdown","metadata":{"id":"G-_RHoP6wXUa"},"source":["## 3) Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"rCg1JeXtwi9C"},"source":["### 1. Unroll TimeStamp column using Fast.ai Tabular Model"]},{"cell_type":"markdown","metadata":{"id":"pdaqEQdnwutS"},"source":["### 2. Categorize variables into independent and dependent variables. In Machine Learning, the result is dependent variable and independent variables also be called as features"]},{"cell_type":"markdown","metadata":{"id":"GE8Uy9rMxtUS"},"source":["#### 2a. Reformat True/False columns to binary scale"]},{"cell_type":"markdown","metadata":{"id":"_lynfudSx5M5"},"source":["## 4) Statiscal Analysis of Data Frame"]},{"cell_type":"markdown","metadata":{"id":"iicaNh50yA4i"},"source":["### 1. Create SQLContext for simple query"]},{"cell_type":"markdown","metadata":{"id":"6xd4rH9ryS8q"},"source":["### 2. Create a temporary table from spark df and perform sql queries. Any queries range from simple select to more complex aggregate is welcome. Create new dataframes from those queries"]},{"cell_type":"markdown","metadata":{"id":"_0rx6rwfy8Dt"},"source":["### 3. Convert Spark df to Pandas df and using pandas-profiling to give a report. You should see a large amount of NaN values, please fill them with 0"]},{"cell_type":"markdown","metadata":{"id":"A60tfX8azRpu"},"source":["# 5) Feature Correlation"]},{"cell_type":"markdown","metadata":{"id":"hoPJ10910Bek"},"source":["### 1. Get correlation matrix values for all values"]},{"cell_type":"markdown","metadata":{"id":"2O4anhIk0Jas"},"source":["### 2. Create a heatmap to show correlation between dependent variable to all categories of features"]},{"cell_type":"markdown","metadata":{"id":"9K5ljgOU1F-t"},"source":["### 3. Identifying highly correlated features with target and other features. Using heatmap to show them all. The threshold is freely defined upon your decision"]},{"cell_type":"markdown","metadata":{"id":"VUo4eTyK1chu"},"source":["### 4. Drop highly correlated featurs. Visualize again using heatmap"]},{"cell_type":"markdown","metadata":{"id":"EknKmW9f2R5A"},"source":["# 6) Build a machine learning pipeline"]},{"cell_type":"markdown","metadata":{"id":"hSGdFiNt2qzd"},"source":["## 1. Using sklearn linear regression and feature selection rfe to estimate feature before putting to sparkml pipeline"]},{"cell_type":"markdown","metadata":{"id":"h-3lQXbp3Ffd"},"source":["## 2. Assemble the feature and transform data"]},{"cell_type":"markdown","metadata":{"id":"ksNhv94x3ce-"},"source":["## 3. Split data into training and testing set"]},{"cell_type":"markdown","metadata":{"id":"6Z0eFWFm3kPG"},"source":["## 4. Fit the linear regression model using pyspark ml"]},{"cell_type":"markdown","metadata":{"id":"r9eV_65m3qve"},"source":["# 7) Test and give the model summary"]}]}