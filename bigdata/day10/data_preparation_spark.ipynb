{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preparation_spark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6H-nouwaeyO"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3J63Uke14M"
      },
      "source": [
        "# `StringIndexer` and `OneHotEncoder`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UJgVwv-atT8"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext(conf=SparkConf())\n",
        "spark = SparkSession(sparkContext=sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m37etkvQa1bj"
      },
      "source": [
        "# Example data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyJ5XVFGatqv",
        "outputId": "64862909-b40e-425f-eed6-6c088e20dc6a"
      },
      "source": [
        "import pandas as pd\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
        "        'y1': [1, 0, 1, 0, 0, 1],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
        "    })\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+---+---+---+---+\n",
            "| x1|    x2| x3| x4| y1| y2|\n",
            "+---+------+---+---+---+---+\n",
            "|  a| apple|  1|2.4|  1|yes|\n",
            "|  a|orange|  1|2.5|  0| no|\n",
            "|  b|orange|  2|3.5|  1| no|\n",
            "|  b|orange|  2|1.4|  0|yes|\n",
            "|  b| peach|  2|2.1|  0|yes|\n",
            "|  c| peach|  4|1.5|  1|yes|\n",
            "+---+------+---+---+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rom_jfRSa9dl"
      },
      "source": [
        "# StringIndexer\n",
        "\n",
        "`StringIndexer` maps a string column to a index column that will be treated as a categorical column by spark. **The indices start with 0 and are ordered by label frequencies**. If it is a numerical column, the column will first be casted to a string column and then indexed by  StringIndexer.\n",
        "\n",
        "There are three steps to implement the StringIndexer\n",
        "\n",
        "1. Build the StringIndexer model: specify the input column and output column names.\n",
        "2. Learn the StringIndexer model: fit the model with your data.\n",
        "3. Execute the indexing: call the transform function to execute the indexing process.\n",
        "\n",
        "### Example: `StringIndex` column \"x1\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr74TF0Oattg",
        "outputId": "df3890bf-2aee-4423-b508-b7790b390a4f"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# build indexer\n",
        "string_indexer = StringIndexer(inputCol='x1', outputCol='indexed_x1')\n",
        "\n",
        "# learn the model\n",
        "string_indexer_model = string_indexer.fit(df)\n",
        "\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model.transform(df)\n",
        "\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+---+---+---+---+----------+\n",
            "| x1|    x2| x3| x4| y1| y2|indexed_x1|\n",
            "+---+------+---+---+---+---+----------+\n",
            "|  a| apple|  1|2.4|  1|yes|       1.0|\n",
            "|  a|orange|  1|2.5|  0| no|       1.0|\n",
            "|  b|orange|  2|3.5|  1| no|       0.0|\n",
            "|  b|orange|  2|1.4|  0|yes|       0.0|\n",
            "|  b| peach|  2|2.1|  0|yes|       0.0|\n",
            "|  c| peach|  4|1.5|  1|yes|       2.0|\n",
            "+---+------+---+---+---+---+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q09eR6hNbJ4X"
      },
      "source": [
        "From the result above, we can see that (a, b, c) in column x1 are converted to (1.0, 0.0, 2.0). They are ordered by their frequencies in column x1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-aFv1K_bJ7H"
      },
      "source": [
        "# OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VhH9d-ybJ-N"
      },
      "source": [
        "**`OneHotEncoder`** converts each categories of a **StringIndexed** column to a `sparse vector`. Each sparse vector has **at most one single active elements** that indicate the category index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or9TkUeXatvv",
        "outputId": "3f9b6022-73f5-4b3e-8a0a-e4af9d64a29f"
      },
      "source": [
        "df_ohe = df.select('x1')\n",
        "df_ohe.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "| x1|\n",
            "+---+\n",
            "|  a|\n",
            "|  a|\n",
            "|  b|\n",
            "|  b|\n",
            "|  b|\n",
            "|  c|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFm04i8AbTfY"
      },
      "source": [
        "### `StringIndex` column 'x1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D21fB0GatzE",
        "outputId": "bf10c1e9-220d-4f7b-d2dd-0dba3e4f844e"
      },
      "source": [
        "df_x1_indexed = StringIndexer(inputCol='x1', outputCol='indexed_x1').fit(df_ohe).transform(df_ohe)\n",
        "df_x1_indexed.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+\n",
            "| x1|indexed_x1|\n",
            "+---+----------+\n",
            "|  a|       1.0|\n",
            "|  a|       1.0|\n",
            "|  b|       0.0|\n",
            "|  b|       0.0|\n",
            "|  b|       0.0|\n",
            "|  c|       2.0|\n",
            "+---+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsHb_WbJbYUn"
      },
      "source": [
        "'x1' has three categories: 'a', 'b' and 'c',  which corresponding string indices 1.0, 0.0 and 2.0, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A26L03v2bYXQ"
      },
      "source": [
        "### Mapping string indices to sparse vectors\n",
        "\n",
        "* Encoding format: 'string index': ['string indices vector size', 'index of string index in string indices vector', **1.0** ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuFjx-kObYai"
      },
      "source": [
        "Here the string indices vector is `[0.0, 1.0, 2.0]`. Therefore, the mapping between string indices and sparse vectors are:\n",
        "* `0.0: [3, [0], [1.0]]`\n",
        "* `1.0: [3, [1], [1.0]]`\n",
        "* `2.0: [3, [2], [1.0]]`\n",
        "\n",
        "After we convert all sparse vectors to dense vectors, we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC9zDWhMat1R",
        "outputId": "34d45563-9ee0-4d1f-e5dc-e818cfaef0c8"
      },
      "source": [
        "from pyspark.ml.linalg import DenseVector, SparseVector, DenseMatrix, SparseMatrix\n",
        "x = [SparseVector(3, {0: 1.0}).toArray()] + \\\n",
        "    [SparseVector(3, {1: 1.0}).toArray()] + \\\n",
        "    [SparseVector(3, {2: 1.0}).toArray()]\n",
        "\n",
        "import numpy as np\n",
        "np.array(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVbgtDBzbpTs"
      },
      "source": [
        "**The obtained matrix is exactly the matrix that we would use to represent our categorical variable in a statistical class**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDYvaXZEbpWq"
      },
      "source": [
        "### One more step to go\n",
        "\n",
        "`OneHotEncoder` by default will drop the last category. So the **string indices vector** becomes `[0.0, 1.0]`, and the mappings between string indices and sparse vectors are:\n",
        "\n",
        "* `0.0: [2, [0], [1.0]]`\n",
        "* `1.0: [2, [1], [1.0]]`\n",
        "* `2.0: [2, [], []]`\n",
        "\n",
        "We use a sparse vector that has **no active element**(basically all elements are 0's) to represent the last category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws0oqTKkbpZK"
      },
      "source": [
        "# Verify\n",
        "\n",
        "### OneHotEncode column 'indexed_x1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZuADf7iat8D"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UozxlA0cb3Nm",
        "outputId": "26434b65-a42b-482a-9162-8bb0d9235b53"
      },
      "source": [
        "OneHotEncoder(inputCol='indexed_x1', outputCol='encoded_x1').fit(df_x1_indexed).transform(df_x1_indexed).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+-------------+\n",
            "| x1|indexed_x1|   encoded_x1|\n",
            "+---+----------+-------------+\n",
            "|  a|       1.0|(2,[1],[1.0])|\n",
            "|  a|       1.0|(2,[1],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  c|       2.0|    (2,[],[])|\n",
            "+---+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YITbqgAgecCF"
      },
      "source": [
        "### Specify to not drop the last category\n",
        "\n",
        "If we choose to not drop the last category, we get the expected results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6y1gArnb3Qw",
        "outputId": "f4e27c7e-c3b0-4ce3-c237-64c6582ae5c2"
      },
      "source": [
        "OneHotEncoder(dropLast=False, inputCol='indexed_x1', outputCol='encoded_x1').fit(df_x1_indexed).transform(df_x1_indexed).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----------+-------------+\n",
            "| x1|indexed_x1|   encoded_x1|\n",
            "+---+----------+-------------+\n",
            "|  a|       1.0|(3,[1],[1.0])|\n",
            "|  a|       1.0|(3,[1],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  c|       2.0|(3,[2],[1.0])|\n",
            "+---+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw65oYrcb3Ss"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddY5V8VXfuxc"
      },
      "source": [
        "# Vector assembler\n",
        "\n",
        "## Example data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph7SsboWb3Vz",
        "outputId": "b58ac00b-6b63-440d-c635-fce8a5681141"
      },
      "source": [
        "import pandas as pd\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
        "        'y1': [1, 0, 1, 0, 0, 1],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
        "    })\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+---+---+---+---+\n",
            "| x1|    x2| x3| x4| y1| y2|\n",
            "+---+------+---+---+---+---+\n",
            "|  a| apple|  1|2.4|  1|yes|\n",
            "|  a|orange|  1|2.5|  0| no|\n",
            "|  b|orange|  2|3.5|  1| no|\n",
            "|  b|orange|  2|1.4|  0|yes|\n",
            "|  b| peach|  2|2.1|  0|yes|\n",
            "|  c| peach|  4|1.5|  1|yes|\n",
            "+---+------+---+---+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6gUwqSJf8h4"
      },
      "source": [
        "# VectorAssembler\n",
        "\n",
        "To fit a ML model in pyspark, we need to combine all feature columns into one single column of vectors: the **featuresCol**. The `VectorAssembler` can be used to combine multiple **`OneHotEncoder` columns** and **other continuous variable columns** into one single column.\n",
        "\n",
        "The example below shows how to combine three OneHotEncoder columns and one numeric column into a **featureCol** column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7oT4vvf-WG"
      },
      "source": [
        "## StringIndex and OneHotEncode categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcAhLCUkb3YB"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGqs4THGb3ak",
        "outputId": "b628cde7-8c9b-4322-f8cb-f66c885bcf1e"
      },
      "source": [
        "all_stages = [StringIndexer(inputCol=c, outputCol='idx_' + c) for c in ['x1', 'x2', 'x3']] + \\\n",
        "             [OneHotEncoder(inputCol='idx_' + c, outputCol='ohe_' + c) for c in ['x1', 'x2', 'x3']]\n",
        "all_stages"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StringIndexer_be84dd2dbc5f,\n",
              " StringIndexer_5a9da8bf79fa,\n",
              " StringIndexer_b2d61a4f22a5,\n",
              " OneHotEncoder_393ad7ef01d2,\n",
              " OneHotEncoder_651f215e54c8,\n",
              " OneHotEncoder_549bff7970bf]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A92-NF0Pat-m",
        "outputId": "cf796d5e-e819-4178-9b97-08795977eb6b"
      },
      "source": [
        "df_new = Pipeline(stages=all_stages).fit(df).transform(df)\n",
        "df_new.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "| x1|    x2| x3| x4| y1| y2|idx_x1|idx_x2|idx_x3|       ohe_x1|       ohe_x2|       ohe_x3|\n",
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "|  a| apple|  1|2.4|  1|yes|   1.0|   2.0|   1.0|(2,[1],[1.0])|    (2,[],[])|(2,[1],[1.0])|\n",
            "|  a|orange|  1|2.5|  0| no|   1.0|   0.0|   1.0|(2,[1],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|\n",
            "|  b|orange|  2|3.5|  1| no|   0.0|   0.0|   0.0|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
            "|  b|orange|  2|1.4|  0|yes|   0.0|   0.0|   0.0|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
            "|  b| peach|  2|2.1|  0|yes|   0.0|   1.0|   0.0|(2,[0],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|\n",
            "|  c| peach|  4|1.5|  1|yes|   2.0|   1.0|   2.0|    (2,[],[])|(2,[1],[1.0])|    (2,[],[])|\n",
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K2_vP5qgOhf"
      },
      "source": [
        "## Assemble feature columns into one single **feacturesCol** with **`VectorAssembler`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "023kdI6bgK8L",
        "outputId": "d5e10366-5962-48cb-9f5c-5726a91cd343"
      },
      "source": [
        "df_assembled = VectorAssembler(inputCols=['ohe_x1', 'ohe_x2', 'ohe_x3', 'x4'], outputCol='featuresCol')\\\n",
        "    .transform(df_new)\\\n",
        "    .drop('idx_x1', 'idx_x2', 'idx_x3')\n",
        "df_assembled.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "|x1 |x2    |x3 |x4 |y1 |y2 |ohe_x1       |ohe_x2       |ohe_x3       |featuresCol                  |\n",
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "|a  |apple |1  |2.4|1  |yes|(2,[1],[1.0])|(2,[],[])    |(2,[1],[1.0])|(7,[1,5,6],[1.0,1.0,2.4])    |\n",
            "|a  |orange|1  |2.5|0  |no |(2,[1],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|[0.0,1.0,1.0,0.0,0.0,1.0,2.5]|\n",
            "|b  |orange|2  |3.5|1  |no |(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|[1.0,0.0,1.0,0.0,1.0,0.0,3.5]|\n",
            "|b  |orange|2  |1.4|0  |yes|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|[1.0,0.0,1.0,0.0,1.0,0.0,1.4]|\n",
            "|b  |peach |2  |2.1|0  |yes|(2,[0],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|[1.0,0.0,0.0,1.0,1.0,0.0,2.1]|\n",
            "|c  |peach |4  |1.5|1  |yes|(2,[],[])    |(2,[1],[1.0])|(2,[],[])    |(7,[3,6],[1.0,1.5])          |\n",
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5loOVu0gTb_"
      },
      "source": [
        "## Convert sparse vectors in featuresCol to dense vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBXnLcxQgLZ7"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kFoNLNEgLco"
      },
      "source": [
        "def dense_features_col(x):\n",
        "    return(x.toArray().dtype)\n",
        "dense_features_col_udf = udf(dense_features_col, returnType=StringType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoHGEeBLgLgw",
        "outputId": "80f202f3-acd6-4ad1-dac5-31d1a05b592b"
      },
      "source": [
        "df_assembled.rdd.map(lambda x: x['featuresCol']).take(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SparseVector(7, {1: 1.0, 5: 1.0, 6: 2.4}),\n",
              " DenseVector([0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.5]),\n",
              " DenseVector([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.5]),\n",
              " DenseVector([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.4])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtYLdUhogZmB",
        "outputId": "c35899a1-f769-4d73-aaaf-01a3b3fad26e"
      },
      "source": [
        "df_assembled.rdd.map(lambda x: list(x['featuresCol'].toArray())).take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.4],\n",
              " [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.5],\n",
              " [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.5],\n",
              " [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.4],\n",
              " [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef9g5bCrgbnn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}